import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.mllib.regression.LinearRegressionModel
import org.apache.spark.mllib.regression.LinearRegressionWithSGD
import org.apache.spark.mllib.linalg.Vectors

// Load and parse the data
val data = sc.textFile("file:/home/training/workspace/groupproject/data/data2.txt")
val parsedData = data.map { line =>
  val parts = line.split(',')
  LabeledPoint(parts(0).toDouble, Vectors.dense(parts.slice(1,16).map(_.toDouble)))
}.cache()

// Build the model
val numIterations = 100
var regression = new LinearRegressionWithSGD().setIntercept(true)
regression.optimizer.setStepSize(0.1).setNumIterations(numIterations)
val model = regression.run(parsedData)


// Evaluate model on training examples and compute training error
val valuesAndPreds = parsedData.map { point =>
  val prediction = model.predict(point.features)
  (point.label, prediction)
}
val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
val SSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.sum()
val ave_v = valuesAndPreds.map{case(v, p) => v}.mean()
val SST = valuesAndPreds.map{case(v, p) => math.pow((v - ave_v), 2)}.sum()
val R2 = 1 - (SSE/SST)
println("R2 = " + R2) 

println(s">>>> Model intercept: ${model.intercept}, weights: ${model.weights}")

//valuesAndPreds.foreach((result) => println(s"actual label: ${result._1}, predicted label: ${result._2}")) 
valuesAndPreds.collect

